{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherited: `data` \n",
    "#%run \"C:\\Users\\Asus\\Desktop\\Coding\\API_Flask\\side_BenckMark\\embedding_Workspace\\bench\\2_Preprocess.ipynb\" \n",
    "\n",
    "import dill\n",
    "# Load session \n",
    "dill.load_session('2_Proprocess.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>combined</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embedding</th>\n",
       "      <th>review_Embedding</th>\n",
       "      <th>semantic_Label</th>\n",
       "      <th>label_Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B003XPF9BO</td>\n",
       "      <td>A3R7JR3FMEBXQB</td>\n",
       "      <td>5</td>\n",
       "      <td>where does one  start...and stop... with a tre...</td>\n",
       "      <td>Wanted to save some to bring to my Chicago fam...</td>\n",
       "      <td>Title: where does one  start...and stop... wit...</td>\n",
       "      <td>52</td>\n",
       "      <td>[0.007018072064965963, -0.02731654793024063, 0...</td>\n",
       "      <td>[0.007018072064965963, -0.02731654793024063, 0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.006393480114638805, -0.008763186633586884,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>297</td>\n",
       "      <td>B003VXHGPK</td>\n",
       "      <td>A21VWSCGW7UUAR</td>\n",
       "      <td>4</td>\n",
       "      <td>Good, but not Wolfgang Puck good</td>\n",
       "      <td>Honestly, I have to admit that I expected a li...</td>\n",
       "      <td>Title: Good, but not Wolfgang Puck good; Conte...</td>\n",
       "      <td>178</td>\n",
       "      <td>[-0.003140551969408989, -0.009995664469897747,...</td>\n",
       "      <td>[-0.003140551969408989, -0.009995664469897747,...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.006393480114638805, -0.008763186633586884,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   ProductId          UserId  Score  \\\n",
       "0           0  B003XPF9BO  A3R7JR3FMEBXQB      5   \n",
       "1         297  B003VXHGPK  A21VWSCGW7UUAR      4   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  where does one  start...and stop... with a tre...   \n",
       "1                   Good, but not Wolfgang Puck good   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Wanted to save some to bring to my Chicago fam...   \n",
       "1  Honestly, I have to admit that I expected a li...   \n",
       "\n",
       "                                            combined  n_tokens  \\\n",
       "0  Title: where does one  start...and stop... wit...        52   \n",
       "1  Title: Good, but not Wolfgang Puck good; Conte...       178   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [0.007018072064965963, -0.02731654793024063, 0...   \n",
       "1  [-0.003140551969408989, -0.009995664469897747,...   \n",
       "\n",
       "                                    review_Embedding semantic_Label  \\\n",
       "0  [0.007018072064965963, -0.02731654793024063, 0...       positive   \n",
       "1  [-0.003140551969408989, -0.009995664469897747,...       positive   \n",
       "\n",
       "                                     label_Embedding  \n",
       "0  [-0.006393480114638805, -0.008763186633586884,...  \n",
       "1  [-0.006393480114638805, -0.008763186633586884,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### `Main Calculations` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Setup` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Classification Report` \n",
    "* methods: \"`create_Report`\", \"`extract_summary_metrics`\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Report(current_Prediction):\n",
    "    global cosine_Report \n",
    "    global title_List, precision_List, recall_List, f1_List, support_List\n",
    "    \n",
    "    # Get the classification report with zero_division=1 to handle division by zero\n",
    "    report = classification_report(data[\"semantic_Label\"], data[current_Prediction], zero_division=1)\n",
    "    \n",
    "    title_List, precision_List, recall_List, f1_List, support_List = [], [], [], [], [] \n",
    "\n",
    "    for line in report.split(\"\\n\")[2:-5]:  # Skip the first two lines and the last five lines of the report\n",
    "        # Split the line by spaces but exclude empty splits\n",
    "        values = [value for value in line.split(\"  \") if value]\n",
    "        \n",
    "        # Extract title\n",
    "        try:\n",
    "            title = values[0].strip()\n",
    "        except:\n",
    "            title = float('NaN')\n",
    "        title_List.append(title)\n",
    "        \n",
    "        # Extract precision\n",
    "        try:\n",
    "            precision = float(values[1])\n",
    "        except:\n",
    "            precision = float('NaN')\n",
    "        precision_List.append(precision)\n",
    "        \n",
    "        # Extract recall\n",
    "        try:\n",
    "            recall = float(values[2])\n",
    "        except:\n",
    "            recall = float('NaN')\n",
    "        recall_List.append(recall)\n",
    "        \n",
    "        # Extract f1-score\n",
    "        try:\n",
    "            f1 = float(values[3])\n",
    "        except:\n",
    "            f1 = float('NaN')\n",
    "        f1_List.append(f1)\n",
    "        \n",
    "        # Extract support\n",
    "        try:\n",
    "            support = int(values[4])\n",
    "        except:\n",
    "            support = float('NaN')\n",
    "        support_List.append(support)\n",
    "\n",
    "    # Create dataframe from the values \n",
    "    cosine_Report = pd.DataFrame({\n",
    "        'title': title_List, \n",
    "        'precision': precision_List, \n",
    "        'recall': recall_List, \n",
    "        'f1': f1_List, \n",
    "        'support': support_List\n",
    "    })    \n",
    "    \n",
    "    return cosine_Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(current_Prediction):\n",
    "    # Get the classification report with zero_division=1 to handle division by zero\n",
    "    report = classification_report(data[\"semantic_Label\"], data[current_Prediction], zero_division=1)\n",
    "    \n",
    "    # Split the report by lines\n",
    "    lines = report.split(\"\\n\")\n",
    "    \n",
    "    # Extract accuracy\n",
    "    try:\n",
    "        accuracy = float(lines[-2].split(\":\")[1].strip())\n",
    "    except:\n",
    "        accuracy = float('NaN')\n",
    "    \n",
    "    # Extract macro avg metrics\n",
    "    macro_avg = lines[-4].split()\n",
    "    try:\n",
    "        macro_precision = float(macro_avg[2])\n",
    "    except:\n",
    "        macro_precision = float('NaN')\n",
    "    try:\n",
    "        macro_recall = float(macro_avg[3])\n",
    "    except:\n",
    "        macro_recall = float('NaN')\n",
    "    try:\n",
    "        macro_f1 = float(macro_avg[4])\n",
    "    except:\n",
    "        macro_f1 = float('NaN')\n",
    "    try:\n",
    "        macro_support = int(macro_avg[5])\n",
    "    except:\n",
    "        macro_support = float('NaN')\n",
    "    \n",
    "    # Extract weighted avg metrics\n",
    "    weighted_avg = lines[-3].split()\n",
    "    try:\n",
    "        weighted_precision = float(weighted_avg[2])\n",
    "    except:\n",
    "        weighted_precision = float('NaN')\n",
    "    try:\n",
    "        weighted_recall = float(weighted_avg[3])\n",
    "    except:\n",
    "        weighted_recall = float('NaN')\n",
    "    try:\n",
    "        weighted_f1 = float(weighted_avg[4])\n",
    "    except:\n",
    "        weighted_f1 = float('NaN')\n",
    "    try:\n",
    "        weighted_support = int(weighted_avg[5])\n",
    "    except:\n",
    "        weighted_support = float('NaN')\n",
    "    \n",
    "    # Create a DataFrame with the extracted metrics\n",
    "    summary_df = pd.DataFrame({\n",
    "        'title': ['accuracy', 'macro avg', 'weighted avg'],\n",
    "        'precision': [float('NaN'), macro_precision, weighted_precision],\n",
    "        'recall': [float('NaN'), macro_recall, weighted_recall],\n",
    "        'f1': [float('NaN'), macro_f1, weighted_f1],\n",
    "        'support': [accuracy, macro_support, weighted_support]\n",
    "    })\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Similarity Calculations`\n",
    "* method: \"`label_Similarity_Score`\", \"`predict_1`\", \"`predict_2`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def label_Similarity_Score(review_embedding, label_embeddings, method='Cosine'):\n",
    "    scores = {}\n",
    "    \n",
    "    for label, embedding in label_embeddings.items():\n",
    "        if method == 'Cosine':\n",
    "            scores[label] = 1 - distance.cosine(review_embedding, embedding)\n",
    "        elif method == 'Euclidean':\n",
    "            scores[label] = 1 / (1 + distance.euclidean(review_embedding, embedding))\n",
    "        elif method == 'Manhattan':\n",
    "            scores[label] = 1 / (1 + distance.cityblock(review_embedding, embedding))\n",
    "        elif method == 'Minkowski':\n",
    "            scores[label] = 1 / (1 + distance.minkowski(review_embedding, embedding, p=3))\n",
    "        elif method == 'Pearson':\n",
    "            scores[label] = np.corrcoef(review_embedding, embedding)[0, 1] # Pearson is not a distance metric, so we use correlation coefficient instead \n",
    "        elif method == 'Canberra':\n",
    "            scores[label] = 1 / (1 + distance.canberra(review_embedding, embedding))\n",
    "       \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def predict_1(similarity_scores, n=1):\n",
    "    # Sort labels based on scores and get the top n labels\n",
    "    sorted_labels = sorted(similarity_scores, key=similarity_scores.get, reverse=True)\n",
    "    return sorted_labels[:n][0]\n",
    "\n",
    "def predict_2(scores):\n",
    "    max_index = np.argmax(scores)\n",
    "    if max_index < len(current_Labels):\n",
    "        return current_Labels[max_index]\n",
    "    else:\n",
    "        return 'External'  # or any other placeholder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Process` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Similarities & Predictions` \n",
    "* Output: \"`Probabilities`\" & \"`Labels`\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Similarity_Score for each review\n",
    "data[\"Similarity_Score_Cosine\"]    = data[\"review_Embedding\"].apply(lambda x: label_Similarity_Score(x, label_Dict, method='Cosine')) \n",
    "data[\"Similarity_Score_Euclidean\"] = data[\"review_Embedding\"].apply(lambda x: label_Similarity_Score(x, label_Dict, method='Euclidean')) \n",
    "data[\"Similarity_Score_Manhattan\"] = data[\"review_Embedding\"].apply(lambda x: label_Similarity_Score(x, label_Dict, method='Manhattan')) \n",
    "data[\"Similarity_Score_Minkowski\"] = data[\"review_Embedding\"].apply(lambda x: label_Similarity_Score(x, label_Dict, method='Minkowski')) \n",
    "data[\"Similarity_Score_Pearson\"]   = data[\"review_Embedding\"].apply(lambda x: label_Similarity_Score(x, label_Dict, method='Pearson')) \n",
    "data[\"Similarity_Score_Canberra\"]  = data[\"review_Embedding\"].apply(lambda x: label_Similarity_Score(x, label_Dict, method='Canberra')) \n",
    "\n",
    "# Assign top 2 labels for each review \n",
    "data[\"predicted_labels_Cosine\"]    = data[\"Similarity_Score_Cosine\"].    apply(lambda x: predict_1(x))\n",
    "data[\"predicted_labels_Euclidean\"] = data[\"Similarity_Score_Euclidean\"]. apply(lambda x: predict_1(x))\n",
    "data[\"predicted_labels_Manhattan\"] = data[\"Similarity_Score_Manhattan\"]. apply(lambda x: predict_1(x))\n",
    "data[\"predicted_labels_Minkowski\"] = data[\"Similarity_Score_Minkowski\"]. apply(lambda x: predict_1(x))\n",
    "data[\"predicted_labels_Pearson\"]   = data[\"Similarity_Score_Pearson\"].   apply(lambda x: predict_1(x))\n",
    "data[\"predicted_labels_Canberra\"]  = data[\"Similarity_Score_Canberra\"].  apply(lambda x: predict_1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### `Checkpoint` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "# Save the entire session\n",
    "dill.dump_session('3_Process.pkl')  \n",
    "# Actialize the session\n",
    "#dill.load_session('session.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
